{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Musat_Bianca_407_Tema3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes for WSD\n",
        "\n",
        "Musat Bianca-Stefania\n",
        "\n",
        "407 Artificial Intelligence"
      ],
      "metadata": {
        "id": "WoRjdlxXeHmT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjOqKMATd-iQ",
        "outputId": "897f8c28-a89c-481f-f045-b515f1b1faa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]   Package senseval is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('senseval')\n",
        "from nltk.corpus import senseval\n",
        "import nltk\n",
        "import random\n",
        "from nltk.classify import accuracy\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import log\n",
        "\n",
        "nltk.download('stopwords')\n",
        "STOPWORDS_SET = nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vocab_frequency(instances, stopwords=STOPWORDS_SET, n=300):\n",
        "    \"\"\"\n",
        "    Given a list of senseval instances, return a list of the n most frequent words that\n",
        "    appears in its context (i.e., the sentence with the target word in), output is in order\n",
        "    of frequency and includes also the number of instances in which that key appears in the\n",
        "    context of instances.\n",
        " \n",
        "    Params\n",
        "    ------\n",
        "    instances: The instances of a word in Senseval2\n",
        "    stopwords: list of stop words\n",
        "    n: number of most frequent words we want to include in the vocabulary\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    A dictionarry containing the words in the vocabulary and their frequences\n",
        "\n",
        "    Source: Laboratory 6, NLP\n",
        "    \"\"\"\n",
        "\n",
        "    fd = nltk.FreqDist()\n",
        "    for i in instances:\n",
        "        (target, suffix) = i.word.split('-')\n",
        "        words = (c[0] for c in i.context if not c[0] == target)\n",
        "        for word in set(words) - set(stopwords):\n",
        "            fd[word] += 1\n",
        "    return fd.most_common()[:n+1]"
      ],
      "metadata": {
        "id": "ibuvUVbuiVwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find what part of speech can be the words we are interested in disambiguating in senseval 2\n",
        "insts = [senseval.instances('line.pos'), senseval.instances('hard.pos'), senseval.instances('serve.pos'), senseval.instances('interest.pos')]\n",
        "poss = []\n",
        "for inst in insts:\n",
        "    for i in inst:\n",
        "        poss.append(i.context[i.position][1])\n",
        "print(set(list(poss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kQ-kdcPZJXe",
        "outputId": "1221b4a5-2c55-4c56-b34e-94aa52270736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'VBG', 'VBZ', 'VBD', 'NNS', 'JJ', 'VB', 'NN'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vocab(instances, stopwords=STOPWORDS_SET, n=300):\n",
        "    \"\"\"\n",
        "    Given a list of senseval instances, return a list of the n most frequent words that\n",
        "    appears in its context.\n",
        " \n",
        "    Params\n",
        "    ------\n",
        "    instances: The instances of a word in Senseval2\n",
        "    stopwords: list of stop words\n",
        "    n: number of most frequent words we want to include in the vocabulary\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    A list containing the words in the vocabulary\n",
        "\n",
        "    Source: Laboratory 6, NLP\n",
        "    \"\"\"\n",
        "    \n",
        "    pos_list = ['VBG', 'VBZ', 'VBD', 'NNS', 'JJ', 'VB', 'NN']\n",
        "    return [w for w,f in extract_vocab_frequency(instances,stopwords,n)] + pos_list"
      ],
      "metadata": {
        "id": "-32MYN-QiWVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wsd_context_features(instance, vocab, dist=3):\n",
        "    \"\"\"\n",
        "    Given a senseval instance, a vocabulary and a window size return the context features associated with that instance.\n",
        " \n",
        "    Params\n",
        "    ------\n",
        "    instance: An instance from Senseval2\n",
        "    vocab: list of words in the vocabulary\n",
        "    dist: window size\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    A dictionary of features\n",
        "\n",
        "    Source: Adapted from Laboratory 6, NLP\n",
        "    \"\"\"\n",
        "\n",
        "    features = {}\n",
        "    ind = instance.position\n",
        "    con = instance.context\n",
        "    for i in range(max(0, ind-dist), ind):\n",
        "        j = ind-i\n",
        "        if con[i][0] in vocab:\n",
        "            features['left-context-word-%s(%s)' % (j, con[i][0])] = features.get('left-context-word-%s(%s)' % (j, con[i][0]), 0) + 1\n",
        "\n",
        "    for i in range(ind+1, min(ind+dist+1, len(con))):\n",
        "        j = i-ind\n",
        "        if con[i][0] in vocab:\n",
        "            features['right-context-word-%s(%s)' % (j, con[i][0])] = features.get('right-context-word-%s(%s)' % (j, con[i][0]), 0) + 1\n",
        "\n",
        "    features[con[ind][1]] = 1 # add the pos of the instance word to the features\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "pYwLXBOBiak4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wsd_word_features(instance, vocab, dist=3):\n",
        "    \"\"\"\n",
        "    Given a senseval instance, a vocabulary and a window size return the word features associated with that instance.\n",
        " \n",
        "    Params\n",
        "    ------\n",
        "    instance: An instance from Senseval2\n",
        "    vocab: list of words in the vocabulary\n",
        "    dist: window size\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    A dictionary of features\n",
        "\n",
        "    Source: Adapted from Laboratory 6, NLP\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    ind = instance.position\n",
        "    con = instance.context\n",
        "    for i in range(max(0, ind-dist), ind):\n",
        "        j = ind-i\n",
        "        if con[i][0] in vocab:\n",
        "            features[con[i][0]] = features.get(con[i][0], 0) + 1\n",
        "\n",
        "    for i in range(ind+1, min(ind+dist+1, len(con))):\n",
        "        j = i-ind\n",
        "        if con[i][0] in vocab:\n",
        "            features[con[i][0]] = features.get(con[i][0], 0) + 1\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "jA3m0VSzicLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_prob_given_sense(context, instances, features_func, sense, vocab, dist=3):\n",
        "    \"\"\"\n",
        "    Given a senseval instance (context), a set of senseval instances (a corpus), a function that computes the features,\n",
        "    a sense, a vocabulary and a window size return the probability of that sense given the context.\n",
        " \n",
        "    Params\n",
        "    ------\n",
        "    context: an instance from Senseval2\n",
        "    instances: A list of instances from Senseval2\n",
        "    features_func: the function that computes the features\n",
        "    sense: the sens we are interested in\n",
        "    vocab: list of words in the vocabulary\n",
        "    dist: window size\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    The probability of the given sense given the context\n",
        "    \"\"\"\n",
        "\n",
        "    features = [features_func(instance, vocab, dist) for instance in instances if instance.senses[0]==sense]\n",
        "    features_contx = features_func(context, vocab, dist)\n",
        "    sum_of_features_contx_occ = 0\n",
        "    for feat in features_contx:\n",
        "        sum = 0\n",
        "        for dict_f in features:\n",
        "            if feat in dict_f:\n",
        "                sum += dict_f[feat]\n",
        "        sum_of_features_contx_occ += sum\n",
        "    if sum_of_features_contx_occ == 0:\n",
        "        return 0\n",
        "    prod = 1\n",
        "    for feat in features_contx:\n",
        "        sum = 0\n",
        "        for dict_f in features:\n",
        "            if feat in dict_f:\n",
        "                sum += dict_f[feat]\n",
        "        prod *= pow((sum / sum_of_features_contx_occ), features_contx[feat])\n",
        "    return prod"
      ],
      "metadata": {
        "id": "DM6lfj8LAfvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sense_probability(instances, sense):\n",
        "    \"\"\"\n",
        "    Computes the probability of a sense, given the instances of a word\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    instances: The instances of a word in Senseval2\n",
        "    sense: The sense we are interested in\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    The probability of the given sense\n",
        "    \"\"\"\n",
        "\n",
        "    sense_len = len([instance for instance in instances if instance.senses[0]==sense])\n",
        "    senses_len = len(instances)\n",
        "    return sense_len / senses_len"
      ],
      "metadata": {
        "id": "2ZBueRYB28FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wsd_classifier(word, features_func, stopwords_list = STOPWORDS_SET):\n",
        "    \"\"\"\n",
        "    Naive bayes classifier for word sense disambiguation. Given a word, the algorithm takes all the instances\n",
        "    of that word in Senseval 2 and creates 3 different sets: a train dataset containing 60% of the instances,\n",
        "    a test dataset containing 20% of the instances and a validation dataset containing the rest 20% of the instances.\n",
        "    We use the validation dataset to pick the optimum values for the vocabulary size and window size. The vocabulary\n",
        "    size is used to pick the most common N words in the training instances (note that we eliminate stop words from\n",
        "    the vocabulary). The window size is used to search in a particular vecinity in the context of the word we are interested\n",
        "    in disambiguating. The alogorithm uses a Naive Bayes classifier to pick the sense that maximize the probability.\n",
        "    After finding the optimum parameters, we apply the Bayes Classifier on the test data and output the accuracy and\n",
        "    the confusion matrix.\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    word: The word we want to disambiguate\n",
        "    features_func: The function used for feature extraction\n",
        "    stopwords_list: The list of stop words we want to exclude from the vocabulary\n",
        "    \"\"\"\n",
        "\n",
        "    # extract the data from the Senseval 2 dataset\n",
        "    events = [(i, i.senses[0]) for i in senseval.instances(word)][:]\n",
        "    senses = list(set(l for (i, l) in events))\n",
        "    instances = [i for (i, l) in events]\n",
        "    instance_labels = [l for (i, l) in events]\n",
        "    print(' Senses: ' + ' '.join(senses))\n",
        "\n",
        "    # split data into train, test and validation\n",
        "    X_train, X_test, y_train, y_test = train_test_split(instances, instance_labels, test_size=0.2, random_state=13)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "    # the vocabulary size and window size values we try at validation\n",
        "    valid_values= {(200, 2):0, (300, 2):0, (300, 3):0, (300, 5):0, (500, 2):0, (500, 3):0, (500, 5):0}\n",
        "\n",
        "    # compute the optimum vocabulary size and window size\n",
        "    for (voc_sz, win_sz) in valid_values:\n",
        "\n",
        "        vocab = extract_vocab(instances, stopwords=stopwords_list, n=voc_sz)\n",
        "\n",
        "        acc = 0\n",
        "        for i, context in enumerate(X_val):\n",
        "            senses_dict = {s:0 for s in senses}\n",
        "            for sense in senses:\n",
        "                senses_dict[sense] = log(sense_probability(X_train, sense) + 0.0001) + log(feature_prob_given_sense(context, X_train, features_func, sense, vocab, win_sz) + 0.0001)\n",
        "            max_elem = max(senses_dict, key=senses_dict.get)\n",
        "            if y_val[i] == max_elem:\n",
        "                acc += 1\n",
        "        valid_values[(voc_sz, win_sz)] = acc / len(X_val)\n",
        "        print(\"For a vocabulary size of\", voc_sz, \"and a window size of\", win_sz, \"we have the accuracy on validation data:\", acc / len(X_val))\n",
        "\n",
        "    (best_voc_sz, best_win_sz) = max(valid_values, key=valid_values.get)\n",
        "\n",
        "    # compute the accuracy and confusion matrix on test data, using the optimum vocabulary size and window size\n",
        "    vocab = extract_vocab(instances, stopwords=stopwords_list, n=best_voc_sz)\n",
        "    y_pred = []\n",
        "    acc = 0\n",
        "    for i, context in enumerate(X_test):\n",
        "        senses_dict = {s:0 for s in senses}\n",
        "        for sense in senses:\n",
        "            senses_dict[sense] = log(sense_probability(X_train, sense) + 0.0001) + log(feature_prob_given_sense(context, X_train, features_func, sense, vocab, best_win_sz) + 0.0001)\n",
        "        max_elem = max(senses_dict, key=senses_dict.get)\n",
        "        y_pred.append(max_elem)\n",
        "        if y_test[i] == max_elem:\n",
        "            acc += 1\n",
        "    print(\"\\nAccuracy on test is: \", acc / len(X_test))\n",
        "\n",
        "    print(\"\\nThe confusion matrix on test data:\\n\")\n",
        "    cm = nltk.ConfusionMatrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "\n",
        "    return X_test, y_test, y_pred"
      ],
      "metadata": {
        "id": "MLkgQ0sb5dVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_context(X_test, y_test, y_pred, sense, no_ex=10):\n",
        "    \"\"\"\n",
        "    Given a senseval instance set (the test set), the true labels (senses), the predicted lables,\n",
        "    a sense to analyse, and the number of examples we want to analyze, print the context of false\n",
        "    positive examples of that sense.\n",
        " \n",
        "    Params\n",
        "    ------\n",
        "    X_test: A list of instances (test set) from Senseval2\n",
        "    y_test: list of true labels (senses)\n",
        "    y_pred: list of predicted labels (senses)\n",
        "    sense: The sense we are interested in\n",
        "    no_ex: The number of examples we want to analyze\n",
        "\n",
        "    Source: Adapted from Laboratory 6, NLP\n",
        "    \"\"\"\n",
        "    true_pos_indx = 2\n",
        "    indx = no_ex\n",
        "    print(\"False positive examples:\")\n",
        "    for i, inst in enumerate(X_test):\n",
        "        if y_pred[i] == sense and y_test[i] != sense and indx > 0:\n",
        "            indx -= 1\n",
        "            p = inst.position\n",
        "            left = ' '.join(w for (w,t) in inst.context[p-3:p])\n",
        "            word = ' '.join(w for (w,t) in inst.context[p:p+1])\n",
        "            right = ' '.join(w for (w,t) in inst.context[p+1:p+4])\n",
        "            senses = ' '.join(inst.senses)\n",
        "            print('%30s |%10s | %-30s -> %s' % (left, word, right, senses))\n",
        "    print(\"True positive examples:\")\n",
        "    for i, inst in enumerate(X_test):\n",
        "        if y_pred[i] == sense and y_test[i] == sense and true_pos_indx > 0:\n",
        "            true_pos_indx -= 1\n",
        "            p = inst.position\n",
        "            left = ' '.join(w for (w,t) in inst.context[p-3:p])\n",
        "            word = ' '.join(w for (w,t) in inst.context[p:p+1])\n",
        "            right = ' '.join(w for (w,t) in inst.context[p+1:p+4])\n",
        "            senses = ' '.join(inst.senses)\n",
        "            print('%30s |%10s | %-30s -> %s' % (left, word, right, senses))"
      ],
      "metadata": {
        "id": "95W6SWeidBWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Features\n",
        "We train the Naive Bayes classifier with word features, consisting of those words from the context of an instance that appear in the vocabulary and how many times they appear in the context."
      ],
      "metadata": {
        "id": "-1P6T88citcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1, y_test1, y_pred1 = wsd_classifier('hard.pos', wsd_word_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p31-ju-dA_x",
        "outputId": "eaa0b769-3a3b-4934-fd88-c5b4da989c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Senses: HARD3 HARD1 HARD2\n",
            "For a vocabulary size of 200 and a window size of 2 we have the accuracy on validation data: 0.7946943483275664\n",
            "For a vocabulary size of 300 and a window size of 2 we have the accuracy on validation data: 0.7946943483275664\n",
            "For a vocabulary size of 300 and a window size of 3 we have the accuracy on validation data: 0.7785467128027682\n",
            "For a vocabulary size of 300 and a window size of 5 we have the accuracy on validation data: 0.7831603229527105\n",
            "For a vocabulary size of 500 and a window size of 2 we have the accuracy on validation data: 0.7946943483275664\n",
            "For a vocabulary size of 500 and a window size of 3 we have the accuracy on validation data: 0.7785467128027682\n",
            "For a vocabulary size of 500 and a window size of 5 we have the accuracy on validation data: 0.7843137254901961\n",
            "\n",
            "Accuracy on test is:  0.825836216839677\n",
            "\n",
            "The confusion matrix on test data:\n",
            "\n",
            "      |   H   H   H |\n",
            "      |   A   A   A |\n",
            "      |   R   R   R |\n",
            "      |   D   D   D |\n",
            "      |   1   2   3 |\n",
            "------+-------------+\n",
            "HARD1 |<696>  7   5 |\n",
            "HARD2 |  66 <16>  . |\n",
            "HARD3 |  73   .  <4>|\n",
            "------+-------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_context(X_test1, y_test1, y_pred1, 'HARD1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfnwzuT8dBJt",
        "outputId": "a38d06d3-24a9-4e49-a0d8-2f51605420f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive examples:\n",
            "                     are a lot |    harder | in oakland than                -> HARD3\n",
            "                  came from `` |      hard | '' assets (                    -> HARD2\n",
            "                               |      hard | work .                         -> HARD2\n",
            "              of diligence and |      hard | work on their                  -> HARD2\n",
            "          de-thatch and aerate |      hard | packed turf areas              -> HARD3\n",
            "      age-changing voice sound |      hard | , and it                       -> HARD2\n",
            "                silky hair and |      hard | bone .                         -> HARD3\n",
            "                cigarette in a |      hard | pack ; continues               -> HARD3\n",
            "                 very dark and |      hard | ferruginous sandstone ,        -> HARD3\n",
            "                    rock and a |      hard | place . ''                     -> HARD3\n",
            "True positive examples:\n",
            "            triathlete was the |   hardest | .                              -> HARD1\n",
            "               could be pretty |      hard | up in the                      -> HARD1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test2, y_test2, y_pred2 = wsd_classifier('line.pos', wsd_word_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cm6FG3CdBQO",
        "outputId": "831853c0-77bf-4caf-fd04-00c5a23d83eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Senses: cord product phone division formation text\n",
            "For a vocabulary size of 200 and a window size of 2 we have the accuracy on validation data: 0.5838359469240049\n",
            "For a vocabulary size of 300 and a window size of 2 we have the accuracy on validation data: 0.5958986731001207\n",
            "For a vocabulary size of 300 and a window size of 3 we have the accuracy on validation data: 0.5910735826296744\n",
            "For a vocabulary size of 300 and a window size of 5 we have the accuracy on validation data: 0.6067551266586249\n",
            "For a vocabulary size of 500 and a window size of 2 we have the accuracy on validation data: 0.5983112183353438\n",
            "For a vocabulary size of 500 and a window size of 3 we have the accuracy on validation data: 0.5922798552472859\n",
            "For a vocabulary size of 500 and a window size of 5 we have the accuracy on validation data: 0.6115802171290712\n",
            "\n",
            "Accuracy on test is:  0.5686746987951807\n",
            "\n",
            "The confusion matrix on test data:\n",
            "\n",
            "          |           f             |\n",
            "          |       d   o             |\n",
            "          |       i   r       p     |\n",
            "          |       v   m       r     |\n",
            "          |       i   a   p   o     |\n",
            "          |   c   s   t   h   d   t |\n",
            "          |   o   i   i   o   u   e |\n",
            "          |   r   o   o   n   c   x |\n",
            "          |   d   n   n   e   t   t |\n",
            "----------+-------------------------+\n",
            "     cord |  <8>  1   5   4  57   1 |\n",
            " division |   . <17>  .   .  56   6 |\n",
            "formation |   2   . <14>  6  51   5 |\n",
            "    phone |   .   2   6 <18> 69   2 |\n",
            "  product |   2   7   5   3<410>  3 |\n",
            "     text |   2   2   3   1  57  <5>|\n",
            "----------+-------------------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_context(X_test2, y_test2, y_pred2, 'product')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmqE_9xZinGM",
        "outputId": "0e285f40-0965-41dd-86c7-427ed13801fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive examples:\n",
            "        polarized along racial |     lines | , black voters                 -> division\n",
            "                 has drawn the |      line | : the back-pay                 -> division\n",
            "                guests wait in |      line | for elevators and              -> formation\n",
            "              the subscriber - |      line | charge , also                  -> phone\n",
            "dedicated telephone communications |     lines | currently exist between        -> phone\n",
            "                   to draw the |      line | ?                              -> division\n",
            "           company 's customer |     lines | increased only 38              -> phone\n",
            "                     to keep a |      line | open for a                     -> phone\n",
            "              breeze on myriad |     lines | high above your                -> cord\n",
            "                has 10 million |     lines | in service ,                   -> phone\n",
            "True positive examples:\n",
            "               its barbie doll |      line | , hot wheels                   -> product\n",
            "       cosmetics and fragrance |     lines | and u .                        -> product\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test3, y_test3, y_pred3 = wsd_classifier('serve.pos', wsd_word_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nj1GQGCh1WD",
        "outputId": "69d9b8e5-c388-4fcb-b146-01171645efbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Senses: SERVE6 SERVE12 SERVE2 SERVE10\n",
            "For a vocabulary size of 200 and a window size of 2 we have the accuracy on validation data: 0.4942922374429224\n",
            "For a vocabulary size of 300 and a window size of 2 we have the accuracy on validation data: 0.519406392694064\n",
            "For a vocabulary size of 300 and a window size of 3 we have the accuracy on validation data: 0.5981735159817352\n",
            "For a vocabulary size of 300 and a window size of 5 we have the accuracy on validation data: 0.6301369863013698\n",
            "For a vocabulary size of 500 and a window size of 2 we have the accuracy on validation data: 0.5468036529680366\n",
            "For a vocabulary size of 500 and a window size of 3 we have the accuracy on validation data: 0.6198630136986302\n",
            "For a vocabulary size of 500 and a window size of 5 we have the accuracy on validation data: 0.6301369863013698\n",
            "\n",
            "Accuracy on test is:  0.6084474885844748\n",
            "\n",
            "The confusion matrix on test data:\n",
            "\n",
            "        |   S   S         |\n",
            "        |   E   E   S   S |\n",
            "        |   R   R   E   E |\n",
            "        |   V   V   R   R |\n",
            "        |   E   E   V   V |\n",
            "        |   1   1   E   E |\n",
            "        |   0   2   2   6 |\n",
            "--------+-----------------+\n",
            "SERVE10 |<350> 44  10   1 |\n",
            "SERVE12 |  99<124> 11   5 |\n",
            " SERVE2 |  90  27 <30>  1 |\n",
            " SERVE6 |  32  21   2 <29>|\n",
            "--------+-----------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_context(X_test3, y_test3, y_pred3, 'SERVE10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moaaaO1LiBoX",
        "outputId": "06df39ac-9b7c-43ff-b4eb-f2efed1a165d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive examples:\n",
            "               risks which had |    served | as justification for           -> SERVE2\n",
            "               actor before he |    served | as mexican ambassador          -> SERVE12\n",
            "                 that the jews |     serve | as a \"                         -> SERVE2\n",
            "           growing wariness of |   serving | on boards of                   -> SERVE12\n",
            "                      . he had |    served | in two world                   -> SERVE12\n",
            "              the islands thus |     serve | only as an                     -> SERVE2\n",
            "           vice chairman after |   serving | as president and               -> SERVE12\n",
            "              these too hardly |     serve | for either physical            -> SERVE2\n",
            "                officer , will |     serve | as acting executive            -> SERVE12\n",
            "                states , which |    serves | a wide area                    -> SERVE6\n",
            "True positive examples:\n",
            "                 olive oil and |     serve | with a hot                     -> SERVE10\n",
            "      thinly sliced sandwiches |    served | in the compound                -> SERVE10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test4, y_test4, y_pred4 = wsd_classifier('interest.pos', wsd_word_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10N4nVXoh1Yi",
        "outputId": "74759cb1-fb3b-4719-d346-8fded56d73e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Senses: interest_5 interest_2 interest_3 interest_1 interest_4 interest_6\n",
            "For a vocabulary size of 200 and a window size of 2 we have the accuracy on validation data: 0.6582278481012658\n",
            "For a vocabulary size of 300 and a window size of 2 we have the accuracy on validation data: 0.6708860759493671\n",
            "For a vocabulary size of 300 and a window size of 3 we have the accuracy on validation data: 0.6856540084388185\n",
            "For a vocabulary size of 300 and a window size of 5 we have the accuracy on validation data: 0.6265822784810127\n",
            "For a vocabulary size of 500 and a window size of 2 we have the accuracy on validation data: 0.6919831223628692\n",
            "For a vocabulary size of 500 and a window size of 3 we have the accuracy on validation data: 0.6940928270042194\n",
            "For a vocabulary size of 500 and a window size of 5 we have the accuracy on validation data: 0.6075949367088608\n",
            "\n",
            "Accuracy on test is:  0.7130801687763713\n",
            "\n",
            "The confusion matrix on test data:\n",
            "\n",
            "           |   i   i   i   i   i   i |\n",
            "           |   n   n   n   n   n   n |\n",
            "           |   t   t   t   t   t   t |\n",
            "           |   e   e   e   e   e   e |\n",
            "           |   r   r   r   r   r   r |\n",
            "           |   e   e   e   e   e   e |\n",
            "           |   s   s   s   s   s   s |\n",
            "           |   t   t   t   t   t   t |\n",
            "           |   _   _   _   _   _   _ |\n",
            "           |   1   2   3   4   5   6 |\n",
            "-----------+-------------------------+\n",
            "interest_1 | <19>  .   .   2   3  46 |\n",
            "interest_2 |   .  <.>  .   .   .   2 |\n",
            "interest_3 |   2   .  <9>  1   1   6 |\n",
            "interest_4 |   2   .   . <10>  2  29 |\n",
            "interest_5 |   2   .   .   2 <41> 32 |\n",
            "interest_6 |   1   .   .   .   3<259>|\n",
            "-----------+-------------------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_context(X_test4, y_test4, y_pred4, 'interest_6')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG_AaM0vidFZ",
        "outputId": "882c0602-4f4c-4504-8684-93c8c147dca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive examples:\n",
            "               , suggesting an |  interest | in looking for                 -> interest_1\n",
            "                      the 49 % |  interest | owned by coca-cola             -> interest_5\n",
            "                    `` now the |  interest | is in what                     -> interest_1\n",
            "                  takes a keen |  interest | in monetary matters            -> interest_1\n",
            "                   15 % voting |  interest | in united ,                    -> interest_5\n",
            "                     's in the |  interest | of the self-regulator          -> interest_4\n",
            "               protect its own | interests | as a shareholder               -> interest_4\n",
            "                  have a great |  interest | in making investments          -> interest_1\n",
            "              that serve those | interests | .                              -> interest_3\n",
            "               addition to its | interests | in las vegas                   -> interest_5\n",
            "True positive examples:\n",
            "                firm 's annual |  interest | payments by $                  -> interest_6\n",
            "            billion of accrued |  interest | this year ,                    -> interest_6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context Features\n",
        "We train the Naive Bayes classifier with context features, consisting of those words and their context (their location in the context) from the context of an instance that appear in the vocabulary. Also, we add the part of speech of the word we want to disambiguate in the features list."
      ],
      "metadata": {
        "id": "m8pxVHGqivwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1, y_test1, y_pred1 = wsd_classifier('hard.pos', wsd_context_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiCMFZTx35Sd",
        "outputId": "714e5449-eb78-492d-fc45-fcc7f06a7341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Senses: HARD3 HARD1 HARD2\n",
            "For a vocabulary size of 200 and a window size of 2 we have the accuracy on validation data: 0.8477508650519031\n",
            "For a vocabulary size of 300 and a window size of 2 we have the accuracy on validation data: 0.8512110726643599\n",
            "For a vocabulary size of 300 and a window size of 3 we have the accuracy on validation data: 0.831603229527105\n",
            "For a vocabulary size of 300 and a window size of 5 we have the accuracy on validation data: 0.8200692041522492\n",
            "For a vocabulary size of 500 and a window size of 2 we have the accuracy on validation data: 0.8558246828143022\n",
            "For a vocabulary size of 500 and a window size of 3 we have the accuracy on validation data: 0.839677047289504\n",
            "For a vocabulary size of 500 and a window size of 5 we have the accuracy on validation data: 0.8154555940023068\n",
            "\n",
            "Accuracy on test is:  0.8650519031141869\n",
            "\n",
            "The confusion matrix on test data:\n",
            "\n",
            "      |   H   H   H |\n",
            "      |   A   A   A |\n",
            "      |   R   R   R |\n",
            "      |   D   D   D |\n",
            "      |   1   2   3 |\n",
            "------+-------------+\n",
            "HARD1 |<703>  4   1 |\n",
            "HARD2 |  49 <33>  . |\n",
            "HARD3 |  55   8 <14>|\n",
            "------+-------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_context(X_test1, y_test1, y_pred1, 'HARD1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXeVUnDlem4a",
        "outputId": "1e7037ef-15bf-485a-f42b-be763cb5f82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive examples:\n",
            "                     are a lot |    harder | in oakland than                -> HARD3\n",
            "                  came from `` |      hard | '' assets (                    -> HARD2\n",
            "          de-thatch and aerate |      hard | packed turf areas              -> HARD3\n",
            "      age-changing voice sound |      hard | , and it                       -> HARD2\n",
            "                silky hair and |      hard | bone .                         -> HARD3\n",
            "                               |      hard | feelings ; james               -> HARD2\n",
            "                 very dark and |      hard | ferruginous sandstone ,        -> HARD3\n",
            "                 consul with a |      hard | top - like                     -> HARD3\n",
            "                      a long , |      hard | look into themselves           -> HARD2\n",
            "                      a long , |      hard | look at how                    -> HARD2\n",
            "True positive examples:\n",
            "            triathlete was the |   hardest | .                              -> HARD1\n",
            "               could be pretty |      hard | up in the                      -> HARD1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test2, y_test2, y_pred2 = wsd_classifier('line.pos', wsd_context_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aNOdYK80572",
        "outputId": "4fbc8eb4-08e6-4811-f018-cd8e569349b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Senses: cord product phone division formation text\n",
            "For a vocabulary size of 200 and a window size of 2 we have the accuracy on validation data: 0.594692400482509\n",
            "For a vocabulary size of 300 and a window size of 2 we have the accuracy on validation data: 0.609167671893848\n",
            "For a vocabulary size of 300 and a window size of 3 we have the accuracy on validation data: 0.6079613992762364\n",
            "For a vocabulary size of 300 and a window size of 5 we have the accuracy on validation data: 0.5910735826296744\n",
            "For a vocabulary size of 500 and a window size of 2 we have the accuracy on validation data: 0.617611580217129\n",
            "For a vocabulary size of 500 and a window size of 3 we have the accuracy on validation data: 0.6127864897466827\n",
            "For a vocabulary size of 500 and a window size of 5 we have the accuracy on validation data: 0.586248492159228\n",
            "\n",
            "Accuracy on test is:  0.5963855421686747\n",
            "\n",
            "The confusion matrix on test data:\n",
            "\n",
            "          |           f             |\n",
            "          |       d   o             |\n",
            "          |       i   r       p     |\n",
            "          |       v   m       r     |\n",
            "          |       i   a   p   o     |\n",
            "          |   c   s   t   h   d   t |\n",
            "          |   o   i   i   o   u   e |\n",
            "          |   r   o   o   n   c   x |\n",
            "          |   d   n   n   e   t   t |\n",
            "----------+-------------------------+\n",
            "     cord |  <4>  .   1   1  70   . |\n",
            " division |   1 <25>  1   .  51   1 |\n",
            "formation |   .   . <22>  1  54   1 |\n",
            "    phone |   .   1   2 <22> 72   . |\n",
            "  product |   .   1   4   5<416>  4 |\n",
            "     text |   1   1   .   5  57  <6>|\n",
            "----------+-------------------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_context(X_test2, y_test2, y_pred2, 'product')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUrRUHL6ig9T",
        "outputId": "79303477-723a-4265-edbb-af5132837a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive examples:\n",
            "        polarized along racial |     lines | , black voters                 -> division\n",
            "                guests wait in |      line | for elevators and              -> formation\n",
            "              the subscriber - |      line | charge , also                  -> phone\n",
            "dedicated telephone communications |     lines | currently exist between        -> phone\n",
            "                  of many long |     lines | is at the                      -> formation\n",
            "              racial and class |     lines | , especially if                -> division\n",
            "           company 's customer |     lines | increased only 38              -> phone\n",
            "              agency draws the |      line | .                              -> division\n",
            "              breeze on myriad |     lines | high above your                -> cord\n",
            "                has 10 million |     lines | in service ,                   -> phone\n",
            "True positive examples:\n",
            "               its barbie doll |      line | , hot wheels                   -> product\n",
            "       cosmetics and fragrance |     lines | and u .                        -> product\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test3, y_test3, y_pred3 = wsd_classifier('serve.pos', wsd_context_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7OUAlSR08-N",
        "outputId": "587df4dc-4025-491e-f1e3-7cec8340e637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Senses: SERVE6 SERVE12 SERVE2 SERVE10\n",
            "For a vocabulary size of 200 and a window size of 2 we have the accuracy on validation data: 0.5639269406392694\n",
            "For a vocabulary size of 300 and a window size of 2 we have the accuracy on validation data: 0.5776255707762558\n",
            "For a vocabulary size of 300 and a window size of 3 we have the accuracy on validation data: 0.591324200913242\n",
            "For a vocabulary size of 300 and a window size of 5 we have the accuracy on validation data: 0.5182648401826484\n",
            "For a vocabulary size of 500 and a window size of 2 we have the accuracy on validation data: 0.593607305936073\n",
            "For a vocabulary size of 500 and a window size of 3 we have the accuracy on validation data: 0.591324200913242\n",
            "For a vocabulary size of 500 and a window size of 5 we have the accuracy on validation data: 0.5\n",
            "\n",
            "Accuracy on test is:  0.6015981735159818\n",
            "\n",
            "The confusion matrix on test data:\n",
            "\n",
            "        |   S   S         |\n",
            "        |   E   E   S   S |\n",
            "        |   R   R   E   E |\n",
            "        |   V   V   R   R |\n",
            "        |   E   E   V   V |\n",
            "        |   1   1   E   E |\n",
            "        |   0   2   2   6 |\n",
            "--------+-----------------+\n",
            "SERVE10 |<378> 14   8   5 |\n",
            "SERVE12 | 143 <91>  2   3 |\n",
            " SERVE2 |  88  20 <38>  2 |\n",
            " SERVE6 |  47  14   3 <20>|\n",
            "--------+-----------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_context(X_test3, y_test3, y_pred3, 'SERVE10')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8WqamN2iHgQ",
        "outputId": "91559f9a-ae56-4265-8e89-d95396ff5e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive examples:\n",
            "               risks which had |    served | as justification for           -> SERVE2\n",
            "          the electric utility |   serving | eastern massachusetts said     -> SERVE6\n",
            "               actor before he |    served | as mexican ambassador          -> SERVE12\n",
            "                 that the jews |     serve | as a \"                         -> SERVE2\n",
            "           growing wariness of |   serving | on boards of                   -> SERVE12\n",
            "                      . he had |    served | in two world                   -> SERVE12\n",
            "              the islands thus |     serve | only as an                     -> SERVE2\n",
            "                    . his wife |    served | on the boards                  -> SERVE12\n",
            "                  of cyprus to |     serve | as a transfer                  -> SERVE2\n",
            "              driving trucks , |   serving | as floorwalkers in             -> SERVE12\n",
            "True positive examples:\n",
            "                 olive oil and |     serve | with a hot                     -> SERVE10\n",
            "      thinly sliced sandwiches |    served | in the compound                -> SERVE10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test4, y_test4, y_pred4 = wsd_classifier('interest.pos', wsd_context_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFF_309-09Ul",
        "outputId": "93080a56-9684-476f-e445-3a2489654325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Senses: interest_5 interest_2 interest_3 interest_1 interest_4 interest_6\n",
            "For a vocabulary size of 200 and a window size of 2 we have the accuracy on validation data: 0.7088607594936709\n",
            "For a vocabulary size of 300 and a window size of 2 we have the accuracy on validation data: 0.7130801687763713\n",
            "For a vocabulary size of 300 and a window size of 3 we have the accuracy on validation data: 0.6708860759493671\n",
            "For a vocabulary size of 300 and a window size of 5 we have the accuracy on validation data: 0.5822784810126582\n",
            "For a vocabulary size of 500 and a window size of 2 we have the accuracy on validation data: 0.7151898734177216\n",
            "For a vocabulary size of 500 and a window size of 3 we have the accuracy on validation data: 0.6329113924050633\n",
            "For a vocabulary size of 500 and a window size of 5 we have the accuracy on validation data: 0.5590717299578059\n",
            "\n",
            "Accuracy on test is:  0.7151898734177216\n",
            "\n",
            "The confusion matrix on test data:\n",
            "\n",
            "           |   i   i   i   i   i   i |\n",
            "           |   n   n   n   n   n   n |\n",
            "           |   t   t   t   t   t   t |\n",
            "           |   e   e   e   e   e   e |\n",
            "           |   r   r   r   r   r   r |\n",
            "           |   e   e   e   e   e   e |\n",
            "           |   s   s   s   s   s   s |\n",
            "           |   t   t   t   t   t   t |\n",
            "           |   _   _   _   _   _   _ |\n",
            "           |   1   2   3   4   5   6 |\n",
            "-----------+-------------------------+\n",
            "interest_1 | <15>  .   .   4   7  44 |\n",
            "interest_2 |   1  <.>  .   .   .   1 |\n",
            "interest_3 |   3   . <10>  .   2   4 |\n",
            "interest_4 |   2   .   . <11> 17  13 |\n",
            "interest_5 |   3   .   .   1 <48> 25 |\n",
            "interest_6 |   .   .   .   2   6<255>|\n",
            "-----------+-------------------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_context(X_test4, y_test4, y_pred4, 'interest_6')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWPWP4wPiJJq",
        "outputId": "836cf896-a563-4020-d5cc-e0514b9bce7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False positive examples:\n",
            "               , suggesting an |  interest | in looking for                 -> interest_1\n",
            "                    `` now the |  interest | is in what                     -> interest_1\n",
            "                  takes a keen |  interest | in monetary matters            -> interest_1\n",
            "                   15 % voting |  interest | in united ,                    -> interest_5\n",
            "                     's in the |  interest | of the self-regulator          -> interest_4\n",
            "                't mention his |  interest | in horse racing                -> interest_1\n",
            "            italy plus certain | interests | in ecuador ,                   -> interest_5\n",
            "                         . 4 % |  interest | in the company                 -> interest_5\n",
            "                       a 100 % |  interest | in the well                    -> interest_5\n",
            "                    not in the |  interest | of the public                  -> interest_4\n",
            "True positive examples:\n",
            "                firm 's annual |  interest | payments by $                  -> interest_6\n",
            "            billion of accrued |  interest | this year ,                    -> interest_6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations:\n",
        "The context features perform better than the word features, proving that specifing the exact position of a word in the context alongside the part of speech of the word to disambiguate is helpful for the Naive Bayes Classifier.\n",
        "\n",
        "All classifiers seem to have a bias towards the most common sense in the dataset. This is expected as the probability of a sense is part of the Naive Bayes formula when computing the most probable sense given a word to disambiguate and a context."
      ],
      "metadata": {
        "id": "itlIgw2sl198"
      }
    }
  ]
}